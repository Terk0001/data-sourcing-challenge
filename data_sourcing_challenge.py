# -*- coding: utf-8 -*-
"""data-sourcing-challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18KwcMhpj_9KIKfaJIJ14bhdMviPiuBMa

### Import Required Libraries and Set Up Environment Variables
"""

from google.colab import drive
drive.mount('/content/drive')

pip install python-dotenv

# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json
import os
import numpy as np
import ast
from datetime import datetime
## Load the NASA_API_KEY from the env file
load_dotenv()
NASA_API_KEY = os.getenv('NASA_API_KEY')

"""### CME Data"""

# Set the base URL to NASA's DONKI API:
base_url = "https://api.nasa.gov/DONKI/"

# Set the specifier for CMEs:
CME = "CME"

# Search for CMEs published between a begin and end date
startDate = "2013-05-01"
endDate   = "2024-05-01"

# Build URL for CME
CME_url = base_url + CME + "?startDate=" + startDate + "&endDate=" + endDate + "&api_key=" + NASA_API_KEY

# Make a "GET" request for the CME URL and store it in a variable named cme_response
cme_response = requests.get(CME_url)

# Convert the response variable to json and store it as a variable named cme_json
cme_json = cme_response.json()

# Preview ONLY the first element from the cme_json list you created in JSON format
# Do NOT print out the entire list
# Use json.dumps with argument indent=4 to format data
print(json.dumps(cme_json[0], indent=5))

# Convert cme_json to a Pandas DataFrame
cme_df = pd.DataFrame(cme_json)
# Keep only the columns: activityID, startTime, linkedEvents
cme_df = cme_df[['activityID', 'startTime', 'linkedEvents']]
cme_df.head()

# Notice that the linkedEvents column allows us to identify the corresponding GST
# Remove rows with missing 'linkedEvents' since we won't be able to assign these to GSTs
cme_df = cme_df.dropna(subset=['linkedEvents'])
cme_df.head()

# Notice that the linkedEvents sometimes contains multiple events per row
# Write a nested for loop that iterates first over each row in the cme DataFrame (using the index)
# and then iterates over the values in 'linkedEvents'
# and adds the elements individually to a list of dictionaries where each row is one element

# Initialize an empty list to store the expanded rows
rows_expanded = []

# Iterate over each index in the DataFrame
for i in cme_df.index:
    # Get the 'linkedEvents' value for the current index
    linked_events = cme_df.loc[i, 'linkedEvents']
    startTime = cme_df.loc[i, 'startTime']
    activityID = cme_df.loc[i, 'activityID']

    # Iterate over each dictionary in the list
    for item in linked_events:
        # Append a new dictionary to the expanded_rows list for each dictionary item and corresponding 'activityID' and 'startTime' value
      rows_expanded.append({'linkedEvents': item, 'startTime': startTime, 'activityID': activityID})
# Create a new DataFrame from the expanded rows
cme_df = pd.DataFrame(rows_expanded)
cme_df.head()

def extract_activityID_from_dict(input_obj):
    processed_dict = None

    # Check if the input is a string and try to parse it
    if isinstance(input_obj, str):
        try:
            # Using ast.literal_eval is safer than eval() for converting string
            # representations of Python literals, including dictionaries.
            processed_dict = ast.literal_eval(input_obj)
        except (ValueError, SyntaxError):
             # If literal_eval fails, it might be a JSON formatted string.
            try:
                processed_dict = json.loads(input_obj)
            except json.JSONDecodeError:
                print(f"Error: Input was a string that could not be parsed. Received: {input_obj}")
                return None
    # If it's already a dictionary, just use it.
    elif isinstance(input_obj, dict):
        processed_dict = input_obj

    # If we have a dictionary now, try to get the value.
    if isinstance(processed_dict, dict):
        return processed_dict.get('activityID')
    else:
        # This will catch cases where the input was not a dict or a parsable string.
        print(f"Error: Input was not a dictionary or a parsable string. Received: {type(input_obj)} with value: {input_obj}")
        return None

# Create a function called extract_activityID_from_dict that takes a dict as input such as in linkedEvents
# and verify below that it works as expected using one row from linkedEvents as an example
# Be sure to use a try and except block to handle errors
def extract_activityID_from_dict(input_obj):
    try:
        activityID = input_obj.get('activityID', None)
        return activityID
    except (ValueError, TypeError) as e:
        # Log the error or print it for debugging
        print(f"Error processing input dictionary: {input_obj}. Error: {e}")
        return None

extract_activityID_from_dict(cme_df.loc[0,'linkedEvents'])

# Apply this function to each row in the 'linkedEvents' column (you can use apply() and a lambda function)
# and create a new column called 'GST_ActivityID' using loc indexer:
cme_df.loc[:, 'GST_ActivityID'] = cme_df['linkedEvents'].apply(lambda x: extract_activityID_from_dict(x))
cme_df.head()

# Remove rows with missing GST_ActivityID, since we can't assign them to GSTs:
cme_df = cme_df.dropna(subset='GST_ActivityID')

# print out the datatype of each column in this DataFrame:
cme_df.info()

# Rename startTime to startTime_CME and activityID to cmeID
cme_df.rename(columns={'startTime':'startTime_CME'}, inplace=True)
cme_df.rename(columns={'activityID':'cmeID'}, inplace=True)
# Convert the 'GST_ActivityID' column to string format
cme_df['GST_ActivityID'] = pd.Series(cme_df['GST_ActivityID'], dtype="string")
# Convert startTime to datetime format
cme_df['startTime_CME'] = pd.to_datetime(cme_df['startTime_CME'])

# Drop linkedEvents

# Verify that all steps were executed correctly
cme_df.info()

# We are only interested in CMEs related to GSTs so keep only rows where the GST_ActivityID column contains 'GST'
# use the method 'contains()' from the str library.
cme_df = cme_df[cme_df['GST_ActivityID'].str.contains('GST')]
cme_df.head()

"""### GST Data"""

# Set the base URL to NASA's DONKI API:
base_url = "https://api.nasa.gov/DONKI/"

# Set the specifier for Geomagnetic Storms (GST):
GST = "GST"

# Search for GSTs between a begin and end date
startDate = "2013-05-01"
endDate   = "2024-05-01"

# Build URL for GST
GST_url = base_url + GST + "?startDate=" + startDate + "&endDate=" + endDate + "&api_key=" + NASA_API_KEY

# Make a "GET" request for the GST URL and store it in a variable named gst_response
get_GST_response = requests.get(GST_url)

# Convert the response variable to json and store it as a variable named gst_json
gst_json = get_GST_response.json()

# Preview ONLY the first element from the gst_json list you created in JSON format
# Do NOT print out the entire list
# Use json.dumps with argument indent=4 to format data
print(json.dumps(gst_json[0], indent=5))

# Convert gst_json to a Pandas DataFrame
gst_df = pd.DataFrame(gst_json)
# Keep only the columns: gstID, startTime, linkedEvents
gst_df = gst_df[['gstID', 'startTime', 'linkedEvents']]
gst_df.head()

# Notice that the linkedEvents column allows us to identify the corresponding CME
# Remove rows with missing 'linkedEvents' since we won't be able to assign these to CME
gst_df = gst_df.dropna(subset=['linkedEvents'])
gst_df.head()

# Notice that the linkedEvents sometimes contains multiple events per row
# Use the explode method to ensure that each row is one element. Ensure to reset the index and drop missing values.
gst_df = gst_df.explode('linkedEvents').reset_index(drop=True).dropna()
gst_df.head()

# Apply the extract_activityID_from_dict function to each row in the 'linkedEvents' column (you can use apply() and a lambda function)
# and create a new column called 'CME_ActivityID' using loc indexer:
gst_df.loc[:, 'CME_ActivityID'] = gst_df['linkedEvents'].apply(lambda x: extract_activityID_from_dict(x))
gst_df.head()
# Remove rows with missing CME_ActivityID, since we can't assign them to CMEs:
gst_df = gst_df.dropna(subset='CME_ActivityID')
gst_df.head()

# Convert the 'CME_ActivityID' column to string format
gst_df['CME_ActivityID'] = pd.Series(gst_df['CME_ActivityID'], dtype="string")
# Convert the 'gstID' column to string format
gst_df['gstID'] = pd.Series(gst_df['gstID'], dtype="string")
# Convert startTime to datetime format
gst_df['startTime_GST'] = pd.to_datetime(gst_df['startTime_GST'])
# Rename startTime to startTime_GST
gst_df.rename(columns={'startTime':'startTime_GST'}, inplace=True)
# Drop linkedEvents

# Verify that all steps were executed correctly
gst_df.info()

# We are only interested in GSTs related to CMEs so keep only rows where the CME_ActivityID column contains 'CME'
# use the method 'contains()' from the str library.
gst_df = gst_df[gst_df['CME_ActivityID'].str.contains('CME')]
gst_df.head()

"""### Merge both datatsets"""

# Now merge both datasets using 'gstID' and 'CME_ActivityID' for gst and 'GST_ActivityID' and 'cmeID' for cme. Use the 'left_on' and 'right_on' specifiers.
final_df = pd.merge(cme_df, gst_df, left_on=['GST_ActivityID', 'cmeID'], right_on=['gstID', 'CME_ActivityID'])
final_df.head()

# Verify that the new DataFrame has the same number of rows as cme and gst
final_df.info()

"""### Computing the time it takes for a CME to cause a GST"""

# Compute the time diff between startTime_GST and startTime_CME by creating a new column called `timeDiff`.
final_df['timeDiff']  = final_df['startTime_GST'] - final_df['startTime_CME']
final_df.head()

# Use describe() to compute the mean and median time
# that it takes for a CME to cause a GST.
final_df[['timeDiff']].describe()

"""### Exporting data in csv format"""

# Export data to CSV without the index
final_df.to_csv('final_df.csv', index=False)